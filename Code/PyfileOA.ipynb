{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6b6c76db",
   "metadata": {},
   "source": [
    "Get the OPENAI-API key from OpenAI website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b5e742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'OPENAI-API key'\n",
    "# Please manually enter OpenAI Key"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7495876e",
   "metadata": {},
   "source": [
    "Get the ACTIVELOOP-API key from OpenAI website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "825def99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activeloop_token = getpass(\"Activeloop Token:\")\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = 'DEEPflake Activeloop API key'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66832770",
   "metadata": {},
   "source": [
    "Use text Loader Loader Module to load my root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2ae3136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "root_dir = \"ROOTDIRECTORY\"\n",
    "\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".py\") and \"/.venv/\" not in dirpath:\n",
    "            try:\n",
    "                loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")\n",
    "                docs.extend(loader.load_and_split())\n",
    "            except Exception as e:\n",
    "                pass\n",
    "print(f\"{len(docs)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97ed072c",
   "metadata": {},
   "source": [
    "Split the text as we are using free version I have created with a size of 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06ca8f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1194, which is longer than the specified 500\n",
      "Created a chunk of size 592, which is longer than the specified 500\n",
      "Created a chunk of size 594, which is longer than the specified 500\n",
      "Created a chunk of size 592, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"{len(texts)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d0a3d03",
   "metadata": {},
   "source": [
    "Initialize OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3de89cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization='', allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e4911d8",
   "metadata": {},
   "source": [
    "Use Deeplake to store all the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e836e9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://venkatavinayvijjapu/text_embedding already exists, loading from the storage\n",
      "Specifying runtime option when loading a Vector Store is not supported and this parameter will be ignored. If you wanted to create a new Vector Store, please specify a path to a Vector Store that does not already exist. To transfer an existing Vector Store to the Managed Tensor Database, use the steps in the link below: (https://docs.activeloop.ai/enterprise-features/managed-database/migrating-datasets-to-the-tensor-database).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://venkatavinayvijjapu/text_embedding', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (437, 1536)  float32   None   \n",
      "    id        text      (437, 1)      str     None   \n",
      " metadata     json      (437, 1)      str     None   \n",
      "   text       text      (437, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.deeplake.DeepLake at 0x12d3131fa90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "db = DeepLake.from_documents(\n",
    "     texts, embeddings, dataset_path=f\"hub://venkatavinayvijjapu/text_embedding\", runtime={\"tensor_db\": True},\n",
    ")\n",
    "db"
   ]
  },
  {
   "cell_type": "raw",
   "id": "135288fc",
   "metadata": {},
   "source": [
    "Read the Emebeddings from DeepFlake and apply few metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32c200a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://venkatavinayvijjapu/text_embedding already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "db = DeepLake(\n",
    "    dataset_path=f\"hub://venkatavinayvijjapu/text_embedding\",\n",
    "    read_only=True,\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba66e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 20\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a30dde11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(x):\n",
    "    # filter based on source code\n",
    "    if \"something\" in x[\"text\"].data()[\"value\"]:\n",
    "        return False\n",
    "\n",
    "    # filter based on path e.g. extension\n",
    "    metadata = x[\"metadata\"].data()[\"value\"]\n",
    "    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n",
    "\n",
    "\n",
    "### turn on below for custom filtering\n",
    "# retriever.search_kwargs['filter'] = filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f62fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67d5c47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What are present in main.py \n",
      "\n",
      "**Answer**: The main.py file contains the following components:\n",
      "\n",
      "- The main() function, which is the entry point of the program.\n",
      "- The load_dotenv() function, which loads environment variables from a .env file.\n",
      "- The st.write() function, which writes HTML code to the Streamlit app.\n",
      "- The st.session_state object, which stores session-specific state variables.\n",
      "- The \"conversation\" and \"chat_history\" variables, which are stored in the session state.\n",
      "- The css variable, which contains HTML code for styling the chat interface.\n",
      "- The YouTube, pafy, and langchain libraries, which are imported for various functionalities.\n",
      "- The bot_template and user_template variables, which contain HTML templates for the chat messages.\n",
      "\n",
      "Please note that the specific functionality and implementation details may vary depending on the code not shown in the provided context. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What are present in main.py\"\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de1489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17001a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
